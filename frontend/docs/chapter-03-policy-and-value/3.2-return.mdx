---
sidebar_position: 2
title: "Lesson 3.2: The Return: Playing the Long Game"
sidebar_label: "3.2 The Return"
---

# Lesson 3.2: The Return: Playing the Long Game

In this lesson, you will learn about the **return**, the mathematical concept that captures the total value a robot can accumulate over time. You will understand how robots weigh immediate rewards against future ones, and why this "long-term thinking" is essential for intelligent behavior.

---

## The Concrete Hook: The Marshmallow Test for Robots

In the famous Stanford "Marshmallow Experiment," children were offered a choice: eat one marshmallow now, or wait 15 minutes and get two marshmallows. The ability to delay gratification for a larger future reward predicted better life outcomes years later.

Robots face the same dilemma constantly:
*   **Option A**: Take a risky shortcut that gives a small reward now, but might lead to a fall (big negative reward) later.
*   **Option B**: Take a stable, slower path that accumulates small rewards consistently over time.

The **return** is how we formalize this trade-off. It's not just about the next step's reward; it's about the *total* reward the robot expects to collect from now until the end of its task.

---

## The Intuition: The Impatient Investor

Imagine you are offered two investment options:

*   **Option A**: Receive $100 today.
*   **Option B**: Receive $110 in one year.

Which is better? It depends on how much you value *future* money compared to *present* money. This is the concept of **discounting**.

For a robot, future rewards are inherently uncertain. The further into the future, the more things can go wrong: the robot might fall, the environment might change, or errors might compound. So we *discount* future rewards, making them worth less than immediate ones.

This is captured by the **discount factor**, denoted by the Greek letter gamma ($\gamma$), a number between 0 and 1.

*   $\gamma = 0$: The robot is completely **myopic**. It only cares about the immediate reward. "Give me the marshmallow NOW!"
*   $\gamma = 1$: The robot is infinitely **patient**. Future rewards are worth exactly as much as present ones. This can cause problems in infinite tasks (the sum diverges).
*   $\gamma = 0.99$: A typical value. The robot is **farsighted** but still prefers sooner rewards. A reward 100 steps in the future is worth $0.99^{100} \approx 0.37$ of its face value.

---

## The Formalism: The Discounted Return

The **return** at time $t$, denoted $G_t$, is the total discounted sum of rewards from time $t$ onwards.

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + \ldots
$$

This can be written more compactly using summation notation:

$$
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

Where:
*   $R_{t+k+1}$ is the reward received at time $t+k+1$.
*   $\gamma \in [0, 1)$ is the discount factor.

### The Recursive Property (Bellman-like)

A key insight is that the return can be defined recursively:

$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

This says: "The return from now is the immediate reward plus the discounted return from the next step." This recursive structure is the foundation of many RL algorithms.

### Finite vs. Infinite Horizons

*   **Episodic Tasks (Finite Horizon)**: The task has a clear end (e.g., a racing game, a pick-and-place task). The sum is finite.
*   **Continuing Tasks (Infinite Horizon)**: The task goes on forever (e.g., a robot operating continuously in a factory). We use $\gamma < 1$ to ensure the sum converges.

---

## The Code: Calculating Returns

Let's implement a function to calculate the return from a sequence of rewards.

```python
import numpy as np

def calculate_return(rewards: list, gamma: float = 0.99) -> float:
    """
    Calculate the discounted return from a list of rewards.
    
    Args:
        rewards: A list of scalar rewards [R_{t+1}, R_{t+2}, ...].
        gamma: The discount factor (0 <= gamma <= 1).
        
    Returns:
        The discounted return G_t.
    """
    G = 0.0
    for k, r in enumerate(rewards):
        G += (gamma ** k) * r
    return G


def calculate_return_recursive(rewards: list, gamma: float = 0.99) -> float:
    """
    Calculate the discounted return using the recursive formula.
    
    G_t = R_{t+1} + gamma * G_{t+1}
    
    Args:
        rewards: A list of scalar rewards.
        gamma: The discount factor.
        
    Returns:
        The discounted return G_t.
    """
    if len(rewards) == 0:
        return 0.0
    return rewards[0] + gamma * calculate_return_recursive(rewards[1:], gamma)


# --- Example Usage ---
if __name__ == "__main__":
    # Example trajectory: robot stays upright for 5 steps, then falls
    rewards = [1.0, 1.0, 1.0, 1.0, 1.0, -10.0, 0.0, 0.0]  # +1 per step, -10 for falling
    
    gamma_values = [0.0, 0.5, 0.9, 0.99]
    
    print("=== Return Calculation Demo ===")
    print(f"Rewards: {rewards}")
    print()
    
    for gamma in gamma_values:
        G = calculate_return(rewards, gamma)
        print(f"gamma = {gamma:.2f} -> Return G_0 = {G:.4f}")
    
    print()
    print("Observation:")
    print("  - With gamma=0.0, only the first reward matters: G = 1.0")
    print("  - With gamma=0.99, the fall penalty (-10) is heavily weighted.")
    print("  - Higher gamma = more 'farsighted' robot.")
```

### Breaking Down the Code

1.  **`calculate_return()`**: Iterates through the reward list, applying the discount factor exponentially. This is the direct implementation of the summation formula.

2.  **`calculate_return_recursive()`**: Implements the recursive definition $G_t = R_{t+1} + \gamma G_{t+1}$. Elegant but less efficient due to function call overhead.

3.  **Example Output**: We compute the return for different discount factors. With $\gamma = 0$, only the first reward counts. With $\gamma = 0.99$, the future fall penalty significantly reduces the return.

---

## The Simulation: Trajectory Comparison

Consider two hypothetical trajectories for our robot:

| Trajectory | Step 1 | Step 2 | Step 3 | Step 4 | Step 5 | Total Reward |
| :--------- | :----: | :----: | :----: | :----: | :----: | :----------: |
| A (Fast, risky) | +5 | +5 | -20 (fall) | 0 | 0 | -10 |
| B (Slow, stable) | +1 | +1 | +1 | +1 | +1 | +5 |

*Without discounting*, Trajectory B has a higher total reward. But what if we discount?

With $\gamma = 0.9$:
*   $G_A = 5 + 0.9 \times 5 + 0.81 \times (-20) = 5 + 4.5 - 16.2 = -6.7$
*   $G_B = 1 + 0.9 + 0.81 + 0.729 + 0.656 = 4.095$

Trajectory B still wins. But if we made the immediate rewards in A much larger, the calculation might flip, illustrating the trade-off between risk and reward that gamma controls.

---

## The Reality Check (Sim-to-Real): Uncertainty in the Future

In the real world, future rewards are uncertain. A robot might *expect* to stay upright, but unexpected disturbances (a push, slippery ground) can change the outcome.

| Simulation | Reality |
| :--------- | :------ |
| Deterministic rewards based on state | Rewards are noisy and environment-dependent |
| Known trajectory length | Unpredictable episode termination |
| Fixed gamma | Gamma might need tuning for deployment stability |

The discount factor $\gamma$ implicitly encodes a "time horizon of trust." A lower $\gamma$ makes the robot more conservative, prioritizing immediate safety. A higher $\gamma$ encourages risk-taking for potentially larger future gains. Tuning $\gamma$ is an important part of deploying RL policies in the real world.

---

## Knowledge Check âœ“

1.  **Question 1**: If $\gamma = 0$, what is the return $G_t$ equal to?
    a) The sum of all future rewards.
    b) Only the immediate reward $R_{t+1}$.
    c) Zero.
    d) Infinity.

2.  **Question 2**: Why do we typically use $\gamma < 1$ for continuing (infinite horizon) tasks?
    a) To make computation faster.
    b) To ensure the sum of discounted rewards converges to a finite value.
    c) Because rewards in the future are always negative.
    d) To simplify the notation.

3.  **Question 3**: Explain in your own words why a robot with $\gamma = 0.5$ might behave differently than one with $\gamma = 0.99$ when faced with a risky but high-reward action.

---

## Try This

**Experiment with Discount Factors**:
1.  Modify the example rewards to simulate a trajectory where the robot walks 10 steps safely (+1 each), then receives a large bonus (+100) at the end.
2.  Calculate the return for $\gamma = 0.5$, $\gamma = 0.9$, and $\gamma = 0.99$.
3.  Which gamma values make the large end bonus "worth" the wait? At what gamma does the immediate small rewards dominate the calculation?

---

## Further Reading

-   Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction* (2nd ed.). MIT Press. (Chapter 3: The Reinforcement Learning Problem - specifically Section 3.3 on Returns and Episodes).
-   Silver, D. (2015). *Lecture 2: Markov Decision Processes*. UCL RL Course. (Available on YouTube, provides excellent visual intuition for discounting).
