"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[386],{9170:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>h,contentTitle:()=>o,default:()=>p,frontMatter:()=>c,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"chapter-03-policy-and-value/3.1-policy","title":"Lesson 3.1: The Policy: How Robots Make Decisions","description":"In this lesson, you will learn what a policy is and why it is the central concept that dictates how an autonomous robot chooses its actions. You will understand the difference between simple \\"reflexes\\" and sophisticated \\"strategies,\\" and implement code that compares these approaches.","source":"@site/docs/chapter-03-policy-and-value/3.1-policy.mdx","sourceDirName":"chapter-03-policy-and-value","slug":"/chapter-03-policy-and-value/3.1-policy","permalink":"/Humanoid-AI-book/chapter-03-policy-and-value/3.1-policy","draft":false,"unlisted":false,"editUrl":"https://github.com/hassanirshad-1/Humanoid-AI-book/tree/main/docs/chapter-03-policy-and-value/3.1-policy.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Lesson 3.1: The Policy: How Robots Make Decisions","sidebar_label":"Lesson 3.1: The Policy"},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2.5: Sparse vs. Dense Rewards","permalink":"/Humanoid-AI-book/chapter-02-reward-signal/2.5-sparse-vs-dense"},"next":{"title":"Lesson 3.2: The Return","permalink":"/Humanoid-AI-book/chapter-03-policy-and-value/3.2-return"}}');var i=n(4848),r=n(8453),t=n(8149);const l='"""\r\nLesson 3.1: Policy Comparison - Random vs. Heuristic\r\n\r\nThis script demonstrates two simple policies for robot control:\r\n1. Random Policy: Selects actions uniformly at random.\r\n2. Heuristic Policy: Uses a proportional controller to counteract tilt.\r\n\r\nUsage:\r\n    Run this script to see the cumulative survival reward for each policy.\r\n    Requires a MuJoCo environment with a humanoid robot (e.g., Unitree G1).\r\n"""\r\n\r\nimport numpy as np\r\n\r\n# --- Configuration ---\r\n# Observation indices (adjust based on your robot\'s observation space)\r\n# These are placeholder indices; adjust to your model\'s obs structure.\r\nPITCH_INDEX = 1  # Index for pitch (forward/backward tilt) in observation\r\nROLL_INDEX = 2   # Index for roll (side-to-side tilt) in observation\r\n\r\n# Heuristic policy gain\r\nKp = 1.5  # Proportional gain for corrective torques\r\n\r\n# Number of actuators (joints) in the robot\r\nNUM_ACTUATORS = 23  # Adjust to your robot\'s action space dimension\r\n\r\n\r\ndef random_policy(observation: np.ndarray) -> np.ndarray:\r\n    """\r\n    Random Policy: Ignores observation and returns random actions.\r\n    \r\n    Args:\r\n        observation: The current state observation from the environment.\r\n        \r\n    Returns:\r\n        A random action vector with values in [-1, 1].\r\n    """\r\n    # Completely ignore the observation\r\n    action = np.random.uniform(-1.0, 1.0, size=NUM_ACTUATORS)\r\n    return action\r\n\r\n\r\ndef heuristic_policy(observation: np.ndarray) -> np.ndarray:\r\n    """\r\n    Heuristic Policy: Simple proportional controller to stay upright.\r\n    \r\n    This policy reads the robot\'s pitch and roll from the observation\r\n    and applies corrective torques to counteract the tilt.\r\n    \r\n    Args:\r\n        observation: The current state observation from the environment.\r\n                     Expected to contain pitch and roll at specified indices.\r\n        \r\n    Returns:\r\n        An action vector with corrective torques applied.\r\n    """\r\n    # Start with zero action\r\n    action = np.zeros(NUM_ACTUATORS)\r\n    \r\n    # Read pitch and roll from observation\r\n    # Pitch > 0 means leaning forward, < 0 means leaning backward\r\n    # Roll > 0 means leaning right, < 0 means leaning left\r\n    pitch = observation[PITCH_INDEX]\r\n    roll = observation[ROLL_INDEX]\r\n    \r\n    # Apply corrective torques\r\n    # If leaning forward (pitch > 0), push hips backward (negative torque)\r\n    # If leaning backward (pitch < 0), push hips forward (positive torque)\r\n    # Similarly for roll affecting ankle/hip abduction\r\n    \r\n    # Simplified: Apply correction to first few actuators (e.g., hip joints)\r\n    # In a real implementation, map these to specific joint indices\r\n    hip_correction = -Kp * pitch\r\n    ankle_correction = -Kp * roll * 0.5  # Smaller gain for roll\r\n    \r\n    # Apply to hip joints (indices 0-5 as a placeholder)\r\n    action[0:6] = hip_correction\r\n    \r\n    # Apply to ankle joints (indices 6-11 as a placeholder)\r\n    action[6:12] = ankle_correction\r\n    \r\n    # Clip to valid range\r\n    action = np.clip(action, -1.0, 1.0)\r\n    \r\n    return action\r\n\r\n\r\ndef compare_policies(env, num_steps: int = 500) -> dict:\r\n    """\r\n    Compare the performance of random vs. heuristic policies.\r\n    \r\n    Args:\r\n        env: A MuJoCo environment with reset() and step() methods.\r\n        num_steps: Number of simulation steps to run.\r\n        \r\n    Returns:\r\n        A dictionary with cumulative rewards for each policy.\r\n    """\r\n    results = {}\r\n    \r\n    policies = {\r\n        "random": random_policy,\r\n        "heuristic": heuristic_policy,\r\n    }\r\n    \r\n    for name, policy_fn in policies.items():\r\n        # Reset environment\r\n        observation = env.reset()\r\n        cumulative_reward = 0.0\r\n        \r\n        for step in range(num_steps):\r\n            # Get action from policy\r\n            action = policy_fn(observation)\r\n            \r\n            # Step the environment\r\n            observation, reward, done, info = env.step(action)\r\n            cumulative_reward += reward\r\n            \r\n            if done:\r\n                print(f"[{name}] Episode ended at step {step}")\r\n                break\r\n        \r\n        results[name] = cumulative_reward\r\n        print(f"[{name}] Cumulative Reward: {cumulative_reward:.2f}")\r\n    \r\n    return results\r\n\r\n\r\n# --- Standalone Demo (no MuJoCo required) ---\r\nif __name__ == "__main__":\r\n    print("=== Policy Comparison Demo ===")\r\n    print()\r\n    \r\n    # Create a fake observation for demonstration\r\n    fake_observation = np.zeros(50)\r\n    fake_observation[PITCH_INDEX] = 0.1  # Slight forward lean\r\n    fake_observation[ROLL_INDEX] = -0.05  # Slight left lean\r\n    \r\n    print("Fake observation (simulating slight forward-left tilt):")\r\n    print(f"  Pitch: {fake_observation[PITCH_INDEX]:.2f} rad")\r\n    print(f"  Roll: {fake_observation[ROLL_INDEX]:.2f} rad")\r\n    print()\r\n    \r\n    # Get actions from each policy\r\n    random_action = random_policy(fake_observation)\r\n    heuristic_action = heuristic_policy(fake_observation)\r\n    \r\n    print("Random Policy Action (first 12 joints):")\r\n    print(f"  {random_action[:12]}")\r\n    print()\r\n    \r\n    print("Heuristic Policy Action (first 12 joints):")\r\n    print(f"  {heuristic_action[:12]}")\r\n    print()\r\n    \r\n    print("Note: The heuristic policy applies structured corrections,")\r\n    print("      while the random policy outputs arbitrary values.")\r\n    print()\r\n    print("To run the full comparison, integrate with your MuJoCo env:")\r\n    print("  results = compare_policies(your_mujoco_env, num_steps=500)")\r\n',c={sidebar_position:1,title:"Lesson 3.1: The Policy: How Robots Make Decisions",sidebar_label:"Lesson 3.1: The Policy"},o="Lesson 3.1: The Policy: How Robots Make Decisions",h={},m=[{value:"The Concrete Hook: Reflex vs. Strategy",id:"the-concrete-hook-reflex-vs-strategy",level:2},{value:"The Intuition: The Robot&#39;s Rulebook",id:"the-intuition-the-robots-rulebook",level:2},{value:"Deterministic vs. Stochastic Policies",id:"deterministic-vs-stochastic-policies",level:3},{value:"The Formalism: The Policy Function",id:"the-formalism-the-policy-function",level:2},{value:"The Code: Random Walk vs. Heuristic Policy",id:"the-code-random-walk-vs-heuristic-policy",level:2},{value:"Breaking Down the Code",id:"breaking-down-the-code",level:3},{value:"The Simulation: Comparing Policy Performance",id:"the-simulation-comparing-policy-performance",level:2},{value:"The Reality Check (Sim-to-Real): From Rules to Noise",id:"the-reality-check-sim-to-real-from-rules-to-noise",level:2},{value:"Knowledge Check \u2713",id:"knowledge-check-",level:2},{value:"Try This",id:"try-this",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const s={annotation:"annotation",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",math:"math",mi:"mi",mn:"mn",mo:"mo",mrow:"mrow",munder:"munder",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"lesson-31-the-policy-how-robots-make-decisions",children:"Lesson 3.1: The Policy: How Robots Make Decisions"})}),"\n",(0,i.jsxs)(s.p,{children:["In this lesson, you will learn what a ",(0,i.jsx)(s.strong,{children:"policy"}),' is and why it is the central concept that dictates how an autonomous robot chooses its actions. You will understand the difference between simple "reflexes" and sophisticated "strategies," and implement code that compares these approaches.']}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-concrete-hook-reflex-vs-strategy",children:"The Concrete Hook: Reflex vs. Strategy"}),"\n",(0,i.jsx)(s.p,{children:"Imagine two drivers approaching a yellow traffic light."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Driver A (The Reflex):"})," Sees yellow, slams the brakes. Always. It's a hard-coded rule: ",(0,i.jsx)(s.code,{children:"IF yellow THEN brake"}),". Simple, predictable, but sometimes dangerous (what if a truck is tailgating?)."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Driver B (The Strategy):"})," Considers the current speed, the distance to the intersection, and even glances in the rearview mirror. Then makes a decision: brake smoothly, or accelerate through. This is a ",(0,i.jsx)(s.em,{children:"context-aware"})," decision."]}),"\n",(0,i.jsxs)(s.p,{children:["In robotics, Driver A follows a ",(0,i.jsx)(s.strong,{children:"deterministic, rule-based policy"}),". Driver B follows a more nuanced, potentially ",(0,i.jsx)(s.strong,{children:"stochastic policy"})," that weighs probabilities and context. This lesson is about understanding and building both."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-intuition-the-robots-rulebook",children:"The Intuition: The Robot's Rulebook"}),"\n",(0,i.jsxs)(s.p,{children:["A ",(0,i.jsx)(s.strong,{children:"policy"})," is simply the robot's rulebook for behavior. It's a mapping from what the robot ",(0,i.jsx)(s.em,{children:"perceives"})," (its state) to what the robot ",(0,i.jsx)(s.em,{children:"does"})," (its action)."]}),"\n",(0,i.jsx)(s.p,{children:"Think of it like this:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.strong,{children:["State (",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"s"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"s"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"})]})})]}),")"]}),': "I am standing, leaning slightly forward, with my left foot in the air."']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.strong,{children:["Action (",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"a"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"a"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"})]})})]}),")"]}),': "Apply 5 Nm of torque to my right hip motor, 2 Nm to my left ankle..."']}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["The policy is the function that connects these two. It answers the question: ",(0,i.jsx)(s.em,{children:'"Given where I am, what should I do?"'})]}),"\n",(0,i.jsx)(s.h3,{id:"deterministic-vs-stochastic-policies",children:"Deterministic vs. Stochastic Policies"}),"\n",(0,i.jsx)(s.p,{children:"There are two main types of policies:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Deterministic Policy (",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"a"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\pi(s) = a"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"})]})]})]}),")"]}),": For every state, there is exactly ",(0,i.jsx)(s.em,{children:"one"})," action to take. It's like a lookup table. Given state ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"s"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"s"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"})]})})]}),", the action is always ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"a"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"a"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"})]})})]}),". This is Driver A."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.em,{children:"Pros"}),": Simple, predictable, fast to execute."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.em,{children:"Cons"}),": Can be exploited by adversaries, may get stuck in loops, doesn't explore."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsxs)(s.strong,{children:["Stochastic Policy (",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"P"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\pi(a|s) = P(a|s)"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"P"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})]})]}),")"]}),": For every state, the policy outputs a ",(0,i.jsx)(s.em,{children:"probability distribution"})," over actions. The robot then ",(0,i.jsx)(s.em,{children:"samples"})," an action from this distribution. This is Driver B making a probabilistic judgment call."]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.em,{children:"Pros"}),": More robust, enables exploration, harder to predict."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.em,{children:"Cons"}),": More complex to represent and learn."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["For our walking robot, a stochastic policy is essential. The real world is noisy and unpredictable. A policy that always does ",(0,i.jsx)(s.em,{children:"exactly"}),' the same thing will fail when the ground is slightly different than expected. A stochastic policy provides the "wiggle room" for adaptation.']}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-formalism-the-policy-function",children:"The Formalism: The Policy Function"}),"\n",(0,i.jsxs)(s.p,{children:["Formally, a policy ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"\u03c0"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\pi"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"})]})})]})," maps states to actions."]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Deterministic Policy:"})}),"\n",(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{children:":"}),(0,i.jsx)(s.mi,{children:"S"}),(0,i.jsx)(s.mo,{children:"\u2192"}),(0,i.jsx)(s.mi,{children:"A"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\pi: S \\rightarrow A"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:":"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"S"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"\u2192"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"A"})]})]})]})}),"\n",(0,i.jsxs)(s.p,{children:["Where ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"S"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"S"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"S"})]})})]})," is the set of all possible states, and ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"A"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"A"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"A"})]})})]})," is the set of all possible actions. Given state ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"s"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"s"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"})]})})]}),", the action is ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"a = \\pi(s)"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})]})]}),"."]}),"\n",(0,i.jsx)(s.p,{children:(0,i.jsx)(s.strong,{children:"Stochastic Policy:"})}),"\n",(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{children:":"}),(0,i.jsx)(s.mi,{children:"S"}),(0,i.jsx)(s.mo,{children:"\xd7"}),(0,i.jsx)(s.mi,{children:"A"}),(0,i.jsx)(s.mo,{children:"\u2192"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"["}),(0,i.jsx)(s.mn,{children:"0"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mn,{children:"1"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"]"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\pi: S \\times A \\rightarrow [0, 1]"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:":"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.7667em",verticalAlign:"-0.0833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05764em"},children:"S"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\xd7"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6833em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"A"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"\u2192"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mopen",children:"["}),(0,i.jsx)(s.span,{className:"mord",children:"0"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"}),(0,i.jsx)(s.span,{className:"mclose",children:"]"})]})]})]})}),"\n",(0,i.jsxs)(s.p,{children:["Here, ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\pi(a|s)"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]})," gives the probability of taking action ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"a"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"a"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"})]})})]})," in state ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"s"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"s"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"})]})})]}),". For any state ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"s"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"s"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"})]})})]}),", the probabilities must sum to 1:"]}),"\n",(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsxs)(s.munder,{children:[(0,i.jsx)(s.mo,{children:"\u2211"}),(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{children:"\u2208"}),(0,i.jsx)(s.mi,{children:"A"})]})]}),(0,i.jsx)(s.mi,{children:"\u03c0"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u2223"}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mn,{children:"1"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\sum_{a \\in A} \\pi(a|s) = 1"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"2.3717em",verticalAlign:"-1.3217em"}}),(0,i.jsx)(s.span,{className:"mop op-limits",children:(0,i.jsxs)(s.span,{className:"vlist-t vlist-t2",children:[(0,i.jsxs)(s.span,{className:"vlist-r",children:[(0,i.jsxs)(s.span,{className:"vlist",style:{height:"1.05em"},children:[(0,i.jsxs)(s.span,{style:{top:"-1.8557em",marginLeft:"0em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3.05em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsxs)(s.span,{className:"mord mtight",children:[(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"a"}),(0,i.jsx)(s.span,{className:"mrel mtight",children:"\u2208"}),(0,i.jsx)(s.span,{className:"mord mathnormal mtight",children:"A"})]})})]}),(0,i.jsxs)(s.span,{style:{top:"-3.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"3.05em"}}),(0,i.jsx)(s.span,{children:(0,i.jsx)(s.span,{className:"mop op-symbol large-op",children:"\u2211"})})]})]}),(0,i.jsx)(s.span,{className:"vlist-s",children:"\u200b"})]}),(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"1.3217em"},children:(0,i.jsx)(s.span,{})})})]})}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.03588em"},children:"\u03c0"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mord",children:"\u2223"}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.6444em"}}),(0,i.jsx)(s.span,{className:"mord",children:"1"})]})]})]})}),"\n",(0,i.jsxs)(s.p,{children:["In practice, for continuous action spaces (like motor torques), we often use probability ",(0,i.jsx)(s.em,{children:"densities"})," rather than discrete probabilities, often represented by a Gaussian (Normal) distribution parameterized by a mean and standard deviation."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-code-random-walk-vs-heuristic-policy",children:"The Code: Random Walk vs. Heuristic Policy"}),"\n",(0,i.jsx)(s.p,{children:"Let's implement two simple policies for our Unitree G1 robot:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Random Policy"}),": At every step, choose a completely random action."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Heuristic Policy"}),": A simple rule-based reflex that tries to stay upright by counteracting tilt."]}),"\n"]}),"\n",(0,i.jsx)(t.A,{language:"python",title:"lesson_3_1_policy_comparison.py",children:l}),"\n",(0,i.jsx)(s.h3,{id:"breaking-down-the-code",children:"Breaking Down the Code"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"random_policy(observation)"})}),": This function ignores the observation entirely and returns a random action vector sampled from a uniform distribution between -1 and 1. This is pure exploration with no exploitation."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"heuristic_policy(observation)"})}),': This function implements a simple proportional controller. It reads the robot\'s pitch (forward/backward tilt) and roll (side-to-side tilt) from the observation and applies corrective torques to the hip and ankle joints. If the robot tilts forward, it pushes back. This is a basic "reflex."']}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Comparison Logic"}),": The main script runs both policies for a fixed number of steps, accumulating a survival reward (from Chapter 2). The policy that keeps the robot upright longer earns a higher score."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-simulation-comparing-policy-performance",children:"The Simulation: Comparing Policy Performance"}),"\n",(0,i.jsx)(s.p,{children:"When you run this code in your MuJoCo environment:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Random Policy"}),": The robot will almost immediately fall over. The random torques provide no stability, and the cumulative reward will be very low."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Heuristic Policy"}),": The robot will likely stay upright for significantly longer. The corrective torques, while simple, actively fight against falling. The cumulative reward will be much higher."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["This demonstrates a core principle: ",(0,i.jsx)(s.strong,{children:"even a simple, hand-crafted policy can vastly outperform random actions"}),". The goal of Reinforcement Learning (Chapter 4 onwards) is to ",(0,i.jsx)(s.em,{children:"learn"})," a policy that is even better than what a human can hand-design."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-reality-check-sim-to-real-from-rules-to-noise",children:"The Reality Check (Sim-to-Real): From Rules to Noise"}),"\n",(0,i.jsx)(s.p,{children:"A heuristic policy like the one above works reasonably well in a clean simulation. But what happens in the real world?"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{style:{textAlign:"left"},children:"Simulation"}),(0,i.jsx)(s.th,{style:{textAlign:"left"},children:"Reality"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Perfect pitch/roll readings from physics engine"}),(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"IMU sensor noise, drift, and calibration errors"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Instantaneous torque application"}),(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Motor latency, gear backlash, friction"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Flat, rigid ground"}),(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Uneven surfaces, soft ground, debris"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Known robot dynamics"}),(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Wear and tear, slightly different joint stiffness"})]})]})]}),"\n",(0,i.jsxs)(s.p,{children:["A heuristic policy tuned for simulation will likely fail when deployed on a real robot. This is where ",(0,i.jsx)(s.em,{children:"learned"})," policies shine: they can be trained in simulation with ",(0,i.jsx)(s.strong,{children:"domain randomization"})," (adding noise and variability) to become robust to real-world conditions. We will explore this in later chapters."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"knowledge-check-",children:"Knowledge Check \u2713"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 1"}),": What is the fundamental difference between a deterministic policy and a stochastic policy?\r\na) Deterministic policies are faster to compute.\r\nb) Stochastic policies output a probability distribution over actions, while deterministic policies output a single action.\r\nc) Stochastic policies can only be used for continuous action spaces.\r\nd) Deterministic policies are always better for robot control."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 2"}),": In the heuristic policy code, why do we multiply the pitch/roll by a gain (",(0,i.jsx)(s.code,{children:"Kp"}),")?\r\na) To convert radians to degrees.\r\nb) To scale the sensor reading into a reasonable motor torque command.\r\nc) Because MuJoCo requires it.\r\nd) To add randomness to the policy."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 3"}),': Explain why a random policy performs poorly for the standing task, even though it technically "explores" all possible actions.']}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"try-this",children:"Try This"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Tune the Heuristic Gains"}),":"]}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["Increase ",(0,i.jsx)(s.code,{children:"Kp"})," significantly (e.g., 5.0 or 10.0). What happens? Does the robot oscillate or become unstable?"]}),"\n",(0,i.jsxs)(s.li,{children:["Decrease ",(0,i.jsx)(s.code,{children:"Kp"})," to a very small value (e.g., 0.1). Does the robot still stand, or does it fall slowly?"]}),"\n",(0,i.jsx)(s.li,{children:"Add a check for torso height in the heuristic policy. If the height drops below a threshold, apply a larger upward force. Does this improve survival time?"}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,i.jsx)(s.em,{children:"Reinforcement Learning: An Introduction"})," (2nd ed.). MIT Press. (Chapter 3: The Reinforcement Learning Problem, and Chapter 13: Policy Gradient Methods)."]}),"\n",(0,i.jsxs)(s.li,{children:["Deisenroth, M. P., Faisal, A. A., & Ong, C. S. (2020). ",(0,i.jsx)(s.em,{children:"Mathematics for Machine Learning"}),". Cambridge University Press. (Chapter 8: When Models Meet Data - for policy function representations)."]}),"\n"]})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);