"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[78],{3396:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>h});const i=JSON.parse('{"id":"chapter-01-intro/1.1-philosophy","title":"Lesson 1.1: Why Did the Robot Fall?","description":"Welcome to the first lesson of your journey into Physical AI. By the end of this chapter, you will understand the fundamental difference between classical robotics and modern AI-driven approaches, and you\'ll have a clear mental model of how robots learn to move.","source":"@site/docs/chapter-01-intro/1.1-philosophy.mdx","sourceDirName":"chapter-01-intro","slug":"/chapter-01-intro/1.1-philosophy","permalink":"/Humanoid-AI-book/chapter-01-intro/1.1-philosophy","draft":false,"unlisted":false,"editUrl":"https://github.com/hassanirshad-1/Humanoid-AI-book/tree/main/docs/chapter-01-intro/1.1-philosophy.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 0.2: The Toolbox (Prerequisites)","permalink":"/Humanoid-AI-book/chapter-00-welcome/0.2-prerequisites"},"next":{"title":"Lesson 1.2: The Body of a Physical AI","permalink":"/Humanoid-AI-book/chapter-01-intro/1.2-hardware-anatomy"}}');var o=t(4848),s=t(8453);const r={},a="Lesson 1.1: Why Did the Robot Fall?",l={},h=[{value:"The Concrete Hook: A Tale of Two Robots",id:"the-concrete-hook-a-tale-of-two-robots",level:2},{value:"The Intuition: Puppets vs. Animals",id:"the-intuition-puppets-vs-animals",level:2},{value:"The Puppet Approach (Classical Robotics)",id:"the-puppet-approach-classical-robotics",level:3},{value:"The Animal Approach (Physical AI)",id:"the-animal-approach-physical-ai",level:3},{value:"The Bicycle Analogy",id:"the-bicycle-analogy",level:3},{value:"The Formalism: The Policy \u03c0(a|s)",id:"the-formalism-the-policy-\u03c0as",level:2},{value:"State (s)",id:"state-s",level:3},{value:"Action (a)",id:"action-a",level:3},{value:"The Policy Function",id:"the-policy-function",level:3},{value:"The Formalism: The Markov Decision Process (MDP)",id:"the-formalism-the-markov-decision-process-mdp",level:2},{value:"Code Representation",id:"code-representation",level:3},{value:"The Goal: Find the Optimal Policy",id:"the-goal-find-the-optimal-policy",level:3},{value:"The Reality Check: The Sim-to-Real Gap",id:"the-reality-check-the-sim-to-real-gap",level:2},{value:"What&#39;s Different in the Real World?",id:"whats-different-in-the-real-world",level:3},{value:"How Do We Bridge the Gap?",id:"how-do-we-bridge-the-gap",level:3},{value:"Knowledge Check \u2713",id:"knowledge-check-",level:2},{value:"Try This: Thought Experiment",id:"try-this-thought-experiment",level:2},{value:"Further Reading",id:"further-reading",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-11-why-did-the-robot-fall",children:"Lesson 1.1: Why Did the Robot Fall?"})}),"\n",(0,o.jsx)(n.p,{children:"Welcome to the first lesson of your journey into Physical AI. By the end of this chapter, you will understand the fundamental difference between classical robotics and modern AI-driven approaches, and you'll have a clear mental model of how robots learn to move."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"the-concrete-hook-a-tale-of-two-robots",children:"The Concrete Hook: A Tale of Two Robots"}),"\n",(0,o.jsx)(n.p,{children:"Watch this comparison carefully. On the left, you see robots from the 2015 DARPA Robotics Challenge\u2014the most advanced robots in the world at that time. They stumble, fall, and move with painful hesitation. On the right, you see videos from 2024: humanoid robots running, jumping, and even doing backflips."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Falling Robot Compilation",src:t(5670).A+"",width:"1024",height:"1024"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:"What changed in just nine years?"})}),"\n",(0,o.jsxs)(n.p,{children:["It wasn't the hardware\u2014motors and sensors have only improved incrementally. It wasn't the physics\u2014gravity is still 9.8 m/s\xb2. The revolution was in the ",(0,o.jsx)(n.em,{children:"software"}),". Specifically, in how we think about robot control."]}),"\n",(0,o.jsxs)(n.p,{children:["The 2015 robots were running ",(0,o.jsx)(n.strong,{children:"hand-coded rules"}),". Engineers spent months writing thousands of lines of code that said things like: \"If the left foot is at angle X and velocity Y, then apply torque Z to the hip.\" This approach is brittle. If the robot encounters a situation the programmer didn't anticipate\u2014a gust of wind, an uneven surface, a slightly different weight distribution\u2014it doesn't know what to do. It falls."]}),"\n",(0,o.jsxs)(n.p,{children:["The 2024 robots are running ",(0,o.jsx)(n.strong,{children:"learned policies"}),'. Instead of telling the robot exactly what to do, we give it a goal ("walk forward without falling") and let it figure out the rest through millions of simulated attempts. The robot develops an intuition for physics that no human could ever program by hand.']}),"\n",(0,o.jsxs)(n.p,{children:["This is the shift from ",(0,o.jsx)(n.strong,{children:"Classical Robotics"})," to ",(0,o.jsx)(n.strong,{children:"Physical AI"}),". And understanding this shift is the key to everything else in this book."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"the-intuition-puppets-vs-animals",children:"The Intuition: Puppets vs. Animals"}),"\n",(0,o.jsx)(n.p,{children:"Let's build an intuition for this difference with an analogy."}),"\n",(0,o.jsx)(n.h3,{id:"the-puppet-approach-classical-robotics",children:"The Puppet Approach (Classical Robotics)"}),"\n",(0,o.jsx)(n.p,{children:"Imagine controlling a marionette puppet. You have strings attached to each limb, and you must consciously pull the right strings in the right sequence to make the puppet walk. Every movement requires your explicit attention. If the stage has a slight slope you didn't expect, the puppet trips because you hadn't planned for that scenario."}),"\n",(0,o.jsx)(n.p,{children:"This is how classical robots work. A human programmer is the puppeteer, and the control code is the strings. The robot can only do exactly what it was told to do, no more, no less."}),"\n",(0,o.jsx)(n.h3,{id:"the-animal-approach-physical-ai",children:"The Animal Approach (Physical AI)"}),"\n",(0,o.jsx)(n.p,{children:'Now think about a cat. A cat can walk on any surface\u2014carpet, tile, grass, gravel\u2014without conscious thought. It can recover from stumbles, adjust to injuries, and navigate obstacles it has never seen before. The cat wasn\'t "programmed" to handle each surface; its brain learned a general strategy for locomotion that generalizes to new situations.'}),"\n",(0,o.jsx)(n.p,{children:"This is what Physical AI aims to achieve. Instead of programming every possible scenario, we train a neural network through experience. The robot develops reflexes and intuitions, much like an animal does."}),"\n",(0,o.jsx)(n.h3,{id:"the-bicycle-analogy",children:"The Bicycle Analogy"}),"\n",(0,o.jsx)(n.p,{children:'Here\'s another way to think about it. Remember learning to ride a bicycle? No one gave you a physics equation for "balance." You didn\'t study the moment of inertia or calculate the optimal handlebar angle. You just... tried. You fell. You tried again. Eventually, after many attempts, your brain built an internal model\u2014a "policy"\u2014that maps "feeling of tilting left" to "turn handlebar left."'}),"\n",(0,o.jsxs)(n.p,{children:["This is ",(0,o.jsx)(n.em,{children:"exactly"})," how we train robots today. We don't tell them ",(0,o.jsx)(n.em,{children:"how"})," to walk. We give them a reward for moving forward and a penalty for falling. Over millions of simulated attempts, they discover optimal strategies that emerge from trial and error."]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"the-formalism-the-policy-\u03c0as",children:"The Formalism: The Policy \u03c0(a|s)"}),"\n",(0,o.jsxs)(n.p,{children:["Now let's get a bit more technical. In the language of AI and machine learning, we call the robot's decision-making strategy a ",(0,o.jsx)(n.strong,{children:"policy"}),". A policy is simply a function that maps the current situation to an action."]}),"\n",(0,o.jsx)(n.p,{children:"We write it as:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u03c0(a|s)\n"})}),"\n",(0,o.jsxs)(n.p,{children:['This reads as: "The policy \u03c0 gives you the probability of taking ',(0,o.jsx)(n.strong,{children:"action"})," ",(0,o.jsx)(n.code,{children:"a"})," given the current ",(0,o.jsx)(n.strong,{children:"state"})," ",(0,o.jsx)(n.code,{children:"s"}),'."']}),"\n",(0,o.jsx)(n.p,{children:"Let's break this down:"}),"\n",(0,o.jsx)(n.h3,{id:"state-s",children:"State (s)"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"state"})," is a complete snapshot of everything the robot knows about itself and the world at a given moment. For a humanoid robot, this typically includes:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Joint positions"}),": The angle of every joint (hip, knee, ankle, shoulder, elbow, etc.)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Joint velocities"}),": How fast each joint is moving"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Body orientation"}),': Which way is "up"? Is the robot tilted?']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact information"}),": Are the feet touching the ground?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Velocity"}),": How fast is the robot's center of mass moving?"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For the Unitree G1 robot we'll use in this book, the state vector might have 50+ dimensions."}),"\n",(0,o.jsx)(n.h3,{id:"action-a",children:"Action (a)"}),"\n",(0,o.jsxs)(n.p,{children:["The ",(0,o.jsx)(n.strong,{children:"action"})," is what the robot actually does\u2014the commands sent to its motors. For each joint, this is typically either:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Torque commands"}),": How much force to apply at each joint"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Position targets"}),": What angle each joint should move toward"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For a 23-joint robot, the action is a 23-dimensional vector, with one value for each motor."}),"\n",(0,o.jsx)(n.h3,{id:"the-policy-function",children:"The Policy Function"}),"\n",(0,o.jsxs)(n.p,{children:["In classical robotics, ",(0,o.jsx)(n.code,{children:"\u03c0"})," was implemented as a massive tree of ",(0,o.jsx)(n.code,{children:"if-then-else"})," statements:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Classical approach (pseudo-code)\r\ndef classical_policy(state):\r\n    if state.phase == "swing_left_leg":\r\n        if state.hip_angle < 0.2:\r\n            return {"left_hip": 0.5, "left_knee": -0.3, ...}\r\n        elif state.hip_angle < 0.4:\r\n            return {"left_hip": 0.3, "left_knee": -0.5, ...}\r\n        # ... hundreds more conditions\r\n    elif state.phase == "stance_left_leg":\r\n        # ... hundreds more conditions\r\n    # ... and so on for every possible state\n'})}),"\n",(0,o.jsxs)(n.p,{children:["In Physical AI, ",(0,o.jsx)(n.code,{children:"\u03c0"})," is a neural network:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Physical AI approach\r\ndef learned_policy(state):\r\n    # A neural network with millions of parameters\r\n    action = neural_network(state)\r\n    return action\n"})}),"\n",(0,o.jsx)(n.p,{children:"The neural network learns the right mapping from states to actions through training, rather than having each case explicitly programmed."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"the-formalism-the-markov-decision-process-mdp",children:"The Formalism: The Markov Decision Process (MDP)"}),"\n",(0,o.jsxs)(n.p,{children:["To be mathematically rigorous, we frame robot control as a ",(0,o.jsx)(n.strong,{children:"Markov Decision Process (MDP)"}),". This is the standard formalism used throughout reinforcement learning."]}),"\n",(0,o.jsxs)(n.p,{children:["An MDP is defined by a five-element tuple: ",(0,o.jsx)(n.code,{children:"(S, A, R, P, \u03b3)"})]}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Symbol"}),(0,o.jsx)(n.th,{children:"Name"}),(0,o.jsx)(n.th,{children:"Description"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"S"})}),(0,o.jsx)(n.td,{children:"State Space"}),(0,o.jsx)(n.td,{children:"The set of all possible states the robot can be in"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"A"})}),(0,o.jsx)(n.td,{children:"Action Space"}),(0,o.jsx)(n.td,{children:"The set of all possible actions the robot can take"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"R"})}),(0,o.jsx)(n.td,{children:"Reward Function"}),(0,o.jsxs)(n.td,{children:[(0,o.jsx)(n.code,{children:"R(s, a)"}),' \u2014 how "good" a state-action pair is']})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"P"})}),(0,o.jsx)(n.td,{children:"Transition Probability"}),(0,o.jsx)(n.td,{children:"`P(s'"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:(0,o.jsx)(n.strong,{children:"\u03b3"})}),(0,o.jsx)(n.td,{children:"Discount Factor"}),(0,o.jsx)(n.td,{children:"How much we value future rewards vs. immediate ones"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"code-representation",children:"Code Representation"}),"\n",(0,o.jsx)(n.p,{children:"Here's how we might represent this in Python:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\n\r\nclass SimpleMDP:\r\n    """\r\n    A conceptual representation of a Markov Decision Process.\r\n    This is for understanding\u2014we\'ll use proper RL libraries later.\r\n    """\r\n    \r\n    def __init__(self, state_dim, action_dim):\r\n        self.state_dim = state_dim    # e.g., 50 for position/velocity\r\n        self.action_dim = action_dim  # e.g., 23 for joint torques\r\n        \r\n    def get_state(self, robot_data):\r\n        """Extract the state vector from sensor data."""\r\n        state = np.concatenate([\r\n            robot_data.joint_positions,    # Shape: (23,)\r\n            robot_data.joint_velocities,   # Shape: (23,)\r\n            robot_data.body_orientation,   # Shape: (4,) quaternion\r\n        ])\r\n        return state\r\n    \r\n    def compute_reward(self, state, action, next_state):\r\n        """\r\n        The reward function\u2014this is where we define our goals.\r\n        Positive reward for moving forward, negative for falling.\r\n        """\r\n        forward_velocity = next_state[0]  # Assume x-velocity is first\r\n        height = next_state[2]            # Assume z-position is third\r\n        \r\n        # Reward moving forward\r\n        reward = forward_velocity * 1.0\r\n        \r\n        # Penalize falling (height too low)\r\n        if height < 0.3:\r\n            reward -= 100  # Big penalty for falling!\r\n        \r\n        # Small penalty for using too much energy\r\n        reward -= 0.01 * np.sum(action ** 2)\r\n        \r\n        return reward\r\n    \r\n    def step(self, state, action):\r\n        """\r\n        Execute one timestep of the simulation.\r\n        In practice, this is done by a physics engine like MuJoCo.\r\n        """\r\n        next_state = physics_engine.simulate(state, action)\r\n        reward = self.compute_reward(state, action, next_state)\r\n        done = self.check_termination(next_state)\r\n        return next_state, reward, done\n'})}),"\n",(0,o.jsx)(n.h3,{id:"the-goal-find-the-optimal-policy",children:"The Goal: Find the Optimal Policy"}),"\n",(0,o.jsxs)(n.p,{children:["The entire goal of robot learning is to find the ",(0,o.jsx)(n.strong,{children:"optimal policy"})," ",(0,o.jsx)(n.code,{children:"\u03c0*"}),"\u2014the policy that maximizes the expected sum of future rewards:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"\u03c0* = argmax_\u03c0 E[ \u03a3 \u03b3^t * R(s_t, a_t) ]\n"})}),"\n",(0,o.jsx)(n.p,{children:"In plain English: find the strategy that gets the most reward over time."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"the-reality-check-the-sim-to-real-gap",children:"The Reality Check: The Sim-to-Real Gap"}),"\n",(0,o.jsxs)(n.p,{children:["Before you get too excited, there's a major challenge we need to address: the ",(0,o.jsx)(n.strong,{children:"sim-to-real gap"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["Training robots in simulation is powerful because we can run millions of trials in hours instead of years. But there's a catch: simulation is ",(0,o.jsx)(n.em,{children:"perfect"}),", and reality is ",(0,o.jsx)(n.em,{children:"messy"}),"."]}),"\n",(0,o.jsx)(n.h3,{id:"whats-different-in-the-real-world",children:"What's Different in the Real World?"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Aspect"}),(0,o.jsx)(n.th,{children:"Simulation"}),(0,o.jsx)(n.th,{children:"Reality"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Sensors"}),(0,o.jsx)(n.td,{children:"Perfect, noise-free"}),(0,o.jsx)(n.td,{children:"Noisy, sometimes failing"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Motors"}),(0,o.jsx)(n.td,{children:"Instant, unlimited"}),(0,o.jsx)(n.td,{children:"Delayed, limited torque"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Ground"}),(0,o.jsx)(n.td,{children:"Perfectly flat"}),(0,o.jsx)(n.td,{children:"Uneven, varying friction"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Physics"}),(0,o.jsx)(n.td,{children:"Simplified model"}),(0,o.jsx)(n.td,{children:"Complex, unknowable details"})]})]})]}),"\n",(0,o.jsx)(n.h3,{id:"how-do-we-bridge-the-gap",children:"How Do We Bridge the Gap?"}),"\n",(0,o.jsx)(n.p,{children:"Researchers use several techniques:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Domain Randomization"}),": During training, randomly vary simulation parameters (friction, mass, motor delays) so the policy learns to handle uncertainty."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"System Identification"}),": Carefully measure the real robot's physics and tune the simulator to match."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Fine-tuning on Real Hardware"}),": Do most training in simulation, then refine with a few real-world trials."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"We'll explore these techniques in later chapters. For now, just remember: what works in simulation doesn't automatically work on a real robot."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"knowledge-check-",children:"Knowledge Check \u2713"}),"\n",(0,o.jsx)(n.p,{children:"Before moving on, make sure you can answer these questions:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"What is the key difference"})," between classical robotics control and Physical AI?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"What does \u03c0(a|s) represent"}),", and what are 's' and 'a'?"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Name the five components"})," of a Markov Decision Process (MDP)."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"What is the sim-to-real gap"}),", and why does it matter?"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"try-this-thought-experiment",children:"Try This: Thought Experiment"}),"\n",(0,o.jsx)(n.p,{children:"Imagine you're designing a policy for a robot that needs to open a door. Think about:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["What would be in the ",(0,o.jsx)(n.strong,{children:"state"})," ",(0,o.jsx)(n.code,{children:"s"}),"? (Hint: What does the robot need to know?)"]}),"\n",(0,o.jsxs)(n.li,{children:["What would be in the ",(0,o.jsx)(n.strong,{children:"action"})," ",(0,o.jsx)(n.code,{children:"a"}),"? (Hint: What can the robot physically do?)"]}),"\n",(0,o.jsxs)(n.li,{children:["What would the ",(0,o.jsx)(n.strong,{children:"reward function"})," look like? (Hint: What's good and what's bad?)"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Write down your answers before the next lesson. We'll revisit this example later."}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsx)(n.p,{children:"If you want to dive deeper into the concepts from this lesson:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Miki, T., et al. (2022). ",(0,o.jsx)(n.em,{children:"Learning robust perceptive locomotion for quadrupedal robots in the wild."})," Science Robotics, 7(62). \u2014 Shows how dogs robots learned to navigate outdoor terrain."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Octo Model Team et al. (2023). ",(0,o.jsx)(n.em,{children:"Octo: An Open-Source General-Purpose Robot Policy."})," arXiv preprint arXiv:2310.08864. \u2014 A modern general-purpose robot policy."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Brohan, A., et al. (2023). ",(0,o.jsx)(n.em,{children:"RT-2: Vision-Language-Action Models with Web-Scale Knowledge."})," Conference on Robot Learning (CoRL). \u2014 How large language models are being combined with robot control."]}),"\n"]}),"\n",(0,o.jsxs)(n.li,{children:["\n",(0,o.jsxs)(n.p,{children:["Sutton, R. S., & Barto, A. G. (2018). ",(0,o.jsx)(n.em,{children:"Reinforcement Learning: An Introduction."})," \u2014 The classic textbook on RL theory."]}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,o.jsx)(n.p,{children:"In the next lesson, we'll zoom in on the robot itself. What sensors does it have? How do its motors work? You'll learn the \"anatomy\" of a humanoid robot so you can understand exactly what your code is controlling."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsxs)(n.strong,{children:["\u2192 Continue to ",(0,o.jsx)(n.a,{href:"/Humanoid-AI-book/chapter-01-intro/1.2-hardware-anatomy",children:"Lesson 1.2: The Body of a Physical AI"})]})})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},5670:(e,n,t)=>{t.d(n,{A:()=>i});const i=t.p+"assets/images/falling_robot_compilation-ce0763d740d47cce50b0a3ffbd7ebf7e.png"},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var i=t(6540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);