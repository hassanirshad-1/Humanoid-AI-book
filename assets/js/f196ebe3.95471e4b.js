"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[608],{4559:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>o,contentTitle:()=>h,default:()=>p,frontMatter:()=>c,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"chapter-02-reward-signal/2.3-reward-shaping","title":"Lesson 2.3: Reward Shaping: Guiding Complex Robot Behaviors","description":"In this lesson, you will delve into reward shaping, a critical technique for designing effective reward functions in Reinforcement Learning. You will learn how to craft more sophisticated reward signals that provide dense, informative feedback to guide a robot towards complex desired behaviors, combining multiple objectives like staying upright, moving forward, and conserving energy.","source":"@site/docs/chapter-02-reward-signal/2.3-reward-shaping.mdx","sourceDirName":"chapter-02-reward-signal","slug":"/chapter-02-reward-signal/2.3-reward-shaping","permalink":"/Humanoid-AI-book/chapter-02-reward-signal/2.3-reward-shaping","draft":false,"unlisted":false,"editUrl":"https://github.com/hassanirshad-1/Humanoid-AI-book/tree/main/docs/chapter-02-reward-signal/2.3-reward-shaping.mdx","tags":[],"version":"current","frontMatter":{"title":"Lesson 2.3: Reward Shaping: Guiding Complex Robot Behaviors","sidebar_label":"2.3 Reward Shaping"},"sidebar":"tutorialSidebar","previous":{"title":"2.2 Your First Reward","permalink":"/Humanoid-AI-book/chapter-02-reward-signal/2.2-first-reward"},"next":{"title":"2.4 Reward Hacking","permalink":"/Humanoid-AI-book/chapter-02-reward-signal/2.4-reward-hacking"}}');var i=n(4848),r=n(8453),t=n(8149);const l='# Copyright 2025 The AI-Native Textbook Authors.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the "License");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an "AS IS" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\n"""\r\nThis script contains a shaped reward function for our humanoid robot.\r\nIt combines multiple reward components to encourage more complex behavior.\r\n"""\r\nimport numpy as np\r\n\r\n# --- Hyperparameters for Reward Components ---\r\n# These weights control the trade-off between different objectives.\r\nUPRIGHT_WEIGHT = 1.0\r\nVELOCITY_WEIGHT = 1.5\r\nCONTROL_COST_WEIGHT = -0.05\r\nDESIRED_VELOCITY = 1.0\r\n\r\ndef is_fallen(d):\r\n  """\r\n  Checks if the robot has fallen based on its torso orientation and height.\r\n  This is the same as in the previous lesson, acting as a "kill condition."\r\n  """\r\n  torso_z_axis = d.xmat[\'torso\', \'zz\']\r\n  torso_z_height = d.qpos[2]\r\n  return torso_z_axis < 0.8 or torso_z_height < 0.3\r\n\r\ndef upright_reward(d):\r\n  """\r\n  Calculates a reward for being upright.\r\n  Uses a sigmoid to create a smooth gradient.\r\n  """\r\n  torso_z_axis = d.xmat[\'torso\', \'zz\']\r\n  # Sigmoid function to smoothly scale reward between 0 and 1.\r\n  return 1 / (1 + np.exp(-6 * (torso_z_axis - 0.9)))\r\n\r\ndef velocity_reward(d):\r\n  """\r\n  Calculates a reward for moving forward at a desired velocity.\r\n  """\r\n  # Velocity along the forward (X) axis.\r\n  forward_velocity = d.qvel[0]\r\n  # Clip the velocity to the desired velocity to prevent rewarding excess speed.\r\n  clipped_velocity = np.clip(forward_velocity, 0, DESIRED_VELOCITY)\r\n  return clipped_velocity\r\n\r\ndef control_cost(d):\r\n  """\r\n  Calculates a penalty for high motor effort.\r\n  """\r\n  # `d.ctrl` contains the torques applied by each actuator.\r\n  # The norm represents the magnitude of the control vector.\r\n  # We square it to penalize large efforts more heavily.\r\n  return np.linalg.norm(d.ctrl) ** 2\r\n\r\ndef calculate_reward(d):\r\n  """\r\n  Calculates the total shaped reward for the current timestep.\r\n\r\n  Args:\r\n    d: MuJoCo data structure.\r\n\r\n  Returns:\r\n    A scalar reward value.\r\n  """\r\n  # If the robot has fallen, the episode is effectively over. Return 0.\r\n  if is_fallen(d):\r\n    return 0.0\r\n\r\n  # Calculate the individual reward components.\r\n  upright = upright_reward(d)\r\n  velocity = velocity_reward(d)\r\n  control = control_cost(d)\r\n\r\n  # Combine them with their respective weights.\r\n  reward = (UPRIGHT_WEIGHT * upright +\r\n            VELOCITY_WEIGHT * velocity +\r\n            CONTROL_COST_WEIGHT * control)\r\n\r\n  return reward\r\n',c={title:"Lesson 2.3: Reward Shaping: Guiding Complex Robot Behaviors",sidebar_label:"2.3 Reward Shaping"},h="Lesson 2.3: Reward Shaping: Guiding Complex Robot Behaviors",o={},d=[{value:"The Concrete Hook: The Inefficient Driver",id:"the-concrete-hook-the-inefficient-driver",level:2},{value:"The Intuition: Breadcrumbs to the Treasure",id:"the-intuition-breadcrumbs-to-the-treasure",level:2},{value:"The Formalism: Potential-Based Reward Shaping",id:"the-formalism-potential-based-reward-shaping",level:2},{value:"The Code: Combining Multiple Rewards for Locomotion",id:"the-code-combining-multiple-rewards-for-locomotion",level:2},{value:"Breaking Down the Code",id:"breaking-down-the-code",level:3},{value:"The Simulation: Visualizing Shaped Learning",id:"the-simulation-visualizing-shaped-learning",level:2},{value:"The Reality Check (Sim-to-Real): The Challenge of Reward Engineering",id:"the-reality-check-sim-to-real-the-challenge-of-reward-engineering",level:2},{value:"Knowledge Check \u2713",id:"knowledge-check-",level:2},{value:"Try This",id:"try-this",level:2},{value:"Further Reading",id:"further-reading",level:2}];function m(e){const s={annotation:"annotation",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",img:"img",li:"li",math:"math",mi:"mi",mo:"mo",mrow:"mrow",msup:"msup",ol:"ol",p:"p",semantics:"semantics",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"lesson-23-reward-shaping-guiding-complex-robot-behaviors",children:"Lesson 2.3: Reward Shaping: Guiding Complex Robot Behaviors"})}),"\n",(0,i.jsxs)(s.p,{children:["In this lesson, you will delve into ",(0,i.jsx)(s.strong,{children:"reward shaping"}),", a critical technique for designing effective reward functions in Reinforcement Learning. You will learn how to craft more sophisticated reward signals that provide dense, informative feedback to guide a robot towards complex desired behaviors, combining multiple objectives like staying upright, moving forward, and conserving energy."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-concrete-hook-the-inefficient-driver",children:"The Concrete Hook: The Inefficient Driver"}),"\n",(0,i.jsxs)(s.p,{children:["Imagine you're trying to teach an autonomous car to drive from point A to point B. If the only reward it gets is a +100 for reaching point B and 0 otherwise, it will take an astronomically long time to learn. The car will likely crash countless times, as it receives no feedback on whether it's moving in the right direction, staying on the road, or maintaining a safe speed. This is the ",(0,i.jsx)(s.strong,{children:"sparse reward problem"}),"."]}),"\n",(0,i.jsx)(s.p,{children:'A much more effective approach would be to give the car smaller rewards: +1 for staying on the road, +5 for moving towards B, -10 for hitting a curb, -50 for a collision, and a small penalty for excessive fuel consumption. These "shaped" rewards provide a continuous stream of feedback, allowing the car to learn much more efficiently. This is precisely the motivation behind reward shaping in robotics.'}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.img,{src:"https://www.forbes.com/advisor/wp-content/uploads/2023/10/how_do_self-driving_cars_work_copy.jpg",alt:"Autonomous car on a track"}),"\r\n",(0,i.jsx)(s.em,{children:"Figure 2.3.1: An autonomous vehicle navigating a track. Effective navigation relies on a well-designed reward function that provides continuous feedback, not just a final destination reward."})]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-intuition-breadcrumbs-to-the-treasure",children:"The Intuition: Breadcrumbs to the Treasure"}),"\n",(0,i.jsx)(s.p,{children:'In Lesson 2.1, we discussed how a reward signal is like playing "Hot or Cold." A sparse reward is like only getting a "You\'re on fire!" when you find the hidden object, with no clues in between. This makes learning incredibly difficult, especially for complex tasks.'}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Reward shaping"})," is analogous to leaving a trail of breadcrumbs (small, frequent rewards) that lead to the final treasure chest (the ultimate goal). Instead of just one large reward at the very end of a long sequence of actions, the agent receives incremental feedback that guides it in the right direction. This makes the learning process faster and more robust."]}),"\n",(0,i.jsx)(s.p,{children:'For a walking robot, we can combine several "breadcrumbs":'}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Uprightness"}),": A small reward for staying balanced (as in Lesson 2.2)."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Progress"}),": A reward for moving towards a goal, often proportional to forward velocity."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Efficiency"}),": A small penalty for using too much energy or making jerky movements."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:['By linearly combining these components, each weighted by its importance, we craft a richer "reward landscape" that explicitly guides the robot\'s exploration and learning process. The robot then learns not just ',(0,i.jsx)(s.em,{children:"what"})," to achieve, but also ",(0,i.jsx)(s.em,{children:"how"})," to achieve it efficiently and safely."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-formalism-potential-based-reward-shaping",children:"The Formalism: Potential-Based Reward Shaping"}),"\n",(0,i.jsxs)(s.p,{children:["One of the most theoretically sound and widely used forms of reward shaping is ",(0,i.jsx)(s.strong,{children:"potential-based reward shaping"}),". This method adds an auxiliary reward ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"F(s, a, s')"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]})," to the environment's true reward ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"R"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"R(s, a, s')"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]}),", such that the optimal policy of the original MDP is preserved."]}),"\n",(0,i.jsxs)(s.p,{children:["The shaped reward ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"R"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"R'(s, a, s')"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]})," is defined as:"]}),"\n",(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"R"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"R"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"+"}),(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"R'(s, a, s') = R(s, a, s') + F(s, a, s')"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0519em",verticalAlign:"-0.25em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8019em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8019em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0519em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.00773em"},children:"R"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8019em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"+"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0519em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8019em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})]})]})}),"\n",(0,i.jsxs)(s.p,{children:["A potential-based shaping function ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"F(s, a, s')"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0019em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.7519em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.063em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]})," takes the form:"]}),"\n",(0,i.jsx)(s.span,{className:"katex-display",children:(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{children:"F"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsx)(s.mi,{children:"a"}),(0,i.jsx)(s.mo,{separator:"true",children:","}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"="}),(0,i.jsx)(s.mi,{children:"\u03b3"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u03a6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsxs)(s.msup,{children:[(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{mathvariant:"normal",lspace:"0em",rspace:"0em",children:"\u2032"})]}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"}),(0,i.jsx)(s.mo,{children:"\u2212"}),(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u03a6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"F(s, a, s') = \\gamma \\Phi(s') - \\Phi(s)"})]})})}),(0,i.jsxs)(s.span,{className:"katex-html","aria-hidden":"true",children:[(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0519em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.13889em"},children:"F"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"a"}),(0,i.jsx)(s.span,{className:"mpunct",children:","}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.1667em"}}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8019em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}}),(0,i.jsx)(s.span,{className:"mrel",children:"="}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2778em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1.0519em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"}),(0,i.jsx)(s.span,{className:"mord",children:"\u03a6"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsxs)(s.span,{className:"mord",children:[(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"msupsub",children:(0,i.jsx)(s.span,{className:"vlist-t",children:(0,i.jsx)(s.span,{className:"vlist-r",children:(0,i.jsx)(s.span,{className:"vlist",style:{height:"0.8019em"},children:(0,i.jsxs)(s.span,{style:{top:"-3.113em",marginRight:"0.05em"},children:[(0,i.jsx)(s.span,{className:"pstrut",style:{height:"2.7em"}}),(0,i.jsx)(s.span,{className:"sizing reset-size6 size3 mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:(0,i.jsx)(s.span,{className:"mord mtight",children:"\u2032"})})})]})})})})})]}),(0,i.jsx)(s.span,{className:"mclose",children:")"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}}),(0,i.jsx)(s.span,{className:"mbin",children:"\u2212"}),(0,i.jsx)(s.span,{className:"mspace",style:{marginRight:"0.2222em"}})]}),(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord",children:"\u03a6"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})]})]})}),"\n",(0,i.jsx)(s.p,{children:"Where:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsxs)(s.mrow,{children:[(0,i.jsx)(s.mi,{mathvariant:"normal",children:"\u03a6"}),(0,i.jsx)(s.mo,{stretchy:"false",children:"("}),(0,i.jsx)(s.mi,{children:"s"}),(0,i.jsx)(s.mo,{stretchy:"false",children:")"})]}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\Phi(s)"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"1em",verticalAlign:"-0.25em"}}),(0,i.jsx)(s.span,{className:"mord",children:"\u03a6"}),(0,i.jsx)(s.span,{className:"mopen",children:"("}),(0,i.jsx)(s.span,{className:"mord mathnormal",children:"s"}),(0,i.jsx)(s.span,{className:"mclose",children:")"})]})})]})," is a ",(0,i.jsx)(s.strong,{children:"potential function"}),' that maps states to scalar values. It represents how "good" a state is.']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"\u03b3"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"\\gamma"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.625em",verticalAlign:"-0.1944em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.05556em"},children:"\u03b3"})]})})]})," is the discount factor."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"This formulation guarantees that the optimal policy of the original (unshaped) MDP remains optimal in the shaped MDP. In simpler terms, if a robot was learning to walk optimally without shaping, it will still learn to walk optimally with potential-based shaping, but potentially much faster."}),"\n",(0,i.jsx)(s.p,{children:'Our practical reward shaping functions often combine multiple components that, when designed carefully, implicitly act like potential functions or their derivatives, providing gradients towards desired sub-goals. For instance, a reward proportional to forward velocity increases as the robot moves closer to a hypothetical "finish line," similar to a potential function decreasing with distance to the goal.'}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-code-combining-multiple-rewards-for-locomotion",children:"The Code: Combining Multiple Rewards for Locomotion"}),"\n",(0,i.jsx)(s.p,{children:"Let's implement a more sophisticated reward function that combines multiple objectives. This composite reward will incentivize staying upright, moving forward, and conserving energy."}),"\n",(0,i.jsx)(t.A,{language:"python",title:"lesson_2_3_shaped_reward.py",children:l}),"\n",(0,i.jsx)(s.h3,{id:"breaking-down-the-code",children:"Breaking Down the Code"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"upright_reward(d)"})}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["This component uses a ",(0,i.jsx)(s.code,{children:"sigmoid"})," function applied to the torso's ",(0,i.jsxs)(s.span,{className:"katex",children:[(0,i.jsx)(s.span,{className:"katex-mathml",children:(0,i.jsx)(s.math,{xmlns:"http://www.w3.org/1998/Math/MathML",children:(0,i.jsxs)(s.semantics,{children:[(0,i.jsx)(s.mrow,{children:(0,i.jsx)(s.mi,{children:"z"})}),(0,i.jsx)(s.annotation,{encoding:"application/x-tex",children:"z"})]})})}),(0,i.jsx)(s.span,{className:"katex-html","aria-hidden":"true",children:(0,i.jsxs)(s.span,{className:"base",children:[(0,i.jsx)(s.span,{className:"strut",style:{height:"0.4306em"}}),(0,i.jsx)(s.span,{className:"mord mathnormal",style:{marginRight:"0.04398em"},children:"z"})]})})]}),'-axis orientation. Instead of an abrupt binary reward (0 or 1), the sigmoid creates a smooth, continuous reward gradient. This smoothness provides a clearer signal to the learning agent, indicating how "close" it is to being perfectly upright, rather than just whether it ',(0,i.jsx)(s.em,{children:"is"})," upright."]}),"\n",(0,i.jsxs)(s.li,{children:["The ",(0,i.jsx)(s.code,{children:"6 * torso_z_axis"})," term scales the input to the sigmoid, making the transition steeper around the upright position."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"velocity_reward(d)"})}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["We extract ",(0,i.jsx)(s.code,{children:"d.qvel[0]"}),", which typically represents the velocity of the torso's center of mass along the robot's forward (X) axis."]}),"\n",(0,i.jsxs)(s.li,{children:["The ",(0,i.jsx)(s.code,{children:"np.clip"}),' function is crucial here. It caps the maximum positive reward for forward velocity. This prevents the robot from developing unstable "super-speed" gaits or disproportionately prioritizing speed over other objectives. It encourages a ',(0,i.jsx)(s.em,{children:"desired"})," range of velocity, not just maximum velocity."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"control_cost(d)"})}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"d.ctrl"})," is a vector representing the control signals (torques) applied to the robot's joints."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"np.linalg.norm(d.ctrl)"})," calculates the magnitude (Euclidean norm) of this control effort."]}),"\n",(0,i.jsxs)(s.li,{children:["The ",(0,i.jsx)(s.code,{children:"**2"})," squares this norm, and it's multiplied by a small negative weight. This component serves as a penalty for high motor activity, encouraging the robot to find smooth, energy-efficient movements rather than jerky, high-torque actions."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:(0,i.jsx)(s.code,{children:"calculate_reward(d)"})}),":"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["This is our master reward function. It first incorporates the critical safety check: if the robot has fallen (using the ",(0,i.jsx)(s.code,{children:"is_fallen"})," check from Lesson 2.2), the reward is immediately ",(0,i.jsx)(s.code,{children:"0.0"}),". This overriding condition ensures that other objectives are secondary to remaining stable."]}),"\n",(0,i.jsxs)(s.li,{children:["If upright, the final reward is a ",(0,i.jsx)(s.strong,{children:"weighted sum"})," of the three components: ",(0,i.jsx)(s.code,{children:"upright_reward"}),", ",(0,i.jsx)(s.code,{children:"velocity_reward"}),", and ",(0,i.jsx)(s.code,{children:"control_cost"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"UPRIGHT_WEIGHT"}),", ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"}),", and ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," are ",(0,i.jsx)(s.strong,{children:"hyperparameters"}),". Tuning these weights is an iterative and often challenging part of reward design, as they dictate the trade-offs between different desired behaviors. For instance, increasing ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"})," will make the robot more eager to move, while increasing ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," will make it more cautious and energy-efficient."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-simulation-visualizing-shaped-learning",children:"The Simulation: Visualizing Shaped Learning"}),"\n",(0,i.jsx)(s.p,{children:"Integrating this shaped reward function into your MuJoCo simulation loop will show a vastly different learning landscape compared to the binary survival reward."}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Continuous Feedback"}),": Instead of only getting a 1 or 0, the robot receives a nuanced score at each step, reflecting its uprightness, forward progress, and control efficiency."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Guided Exploration"}),': Even with a random policy initially, the small positive rewards for moving slightly forward or staying stable help "nudge" the agent in the right direction. An actual learning algorithm would use this richer signal to more effectively explore and converge on a walking gait.']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Trade-offs"}),": As you integrate this and observe a learning agent, you'll see how changing the weights directly influences the robot's emergent behavior. A higher ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"})," might lead to a faster but less stable walk, while a higher ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," could result in a slow but very energy-efficient gait."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"The simulation acts as your laboratory to test hypotheses about how different reward components and their weightings translate into observable robot behaviors."}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"the-reality-check-sim-to-real-the-challenge-of-reward-engineering",children:"The Reality Check (Sim-to-Real): The Challenge of Reward Engineering"}),"\n",(0,i.jsxs)(s.p,{children:["Reward shaping is incredibly powerful, but it introduces its own set of Sim-to-Real challenges, primarily in ",(0,i.jsx)(s.strong,{children:"reward engineering"})," and the ",(0,i.jsx)(s.strong,{children:"calibration of weights"}),"."]}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{style:{textAlign:"left"},children:"Simulation Reward Shaping"}),(0,i.jsx)(s.th,{style:{textAlign:"left"},children:"Reality Reward Engineering Challenges"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Precise state variables (velocity, torque)"}),(0,i.jsxs)(s.td,{style:{textAlign:"left"},children:[(0,i.jsx)(s.strong,{children:"Noisy Sensor Data"}),": True velocity and motor torques are estimates in the real world, not perfect values. This noise propagates into the reward calculation, making it less reliable."]})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Deterministic outcomes for state-action pairs"}),(0,i.jsxs)(s.td,{style:{textAlign:"left"},children:[(0,i.jsx)(s.strong,{children:"Stochasticity"}),": Real-world dynamics are inherently stochastic due to friction, unmodeled physics, and environmental variations. The same action might yield slightly different state transitions and thus different rewards, making learning harder."]})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Easy weight tuning"}),(0,i.jsxs)(s.td,{style:{textAlign:"left"},children:[(0,i.jsx)(s.strong,{children:"Real-World Cost"}),": Tuning weights through trial-and-error on a real robot is expensive, time-consuming, and potentially damaging to hardware. Iterations are slow."]})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Clear definition of objectives"}),(0,i.jsxs)(s.td,{style:{textAlign:"left"},children:[(0,i.jsx)(s.strong,{children:"Measurement Difficulty"}),': Some reward components (e.g., "comfort of gait," "human-like motion") are subjective and very difficult to quantify and measure precisely in the real world.']})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{style:{textAlign:"left"},children:"Negligible latency"}),(0,i.jsxs)(s.td,{style:{textAlign:"left"},children:[(0,i.jsx)(s.strong,{children:"Control Loop Latency"}),": Delays in sensing, computation, and actuation mean the reward signal might not precisely reflect the instantaneous action, leading to a disconnect."]})]})]})]}),"\n",(0,i.jsxs)(s.p,{children:["The art of designing robust shaped rewards for real-world robotics involves careful consideration of sensor limitations, real-world variability, and iterative testing, often with domain experts. Advanced techniques like ",(0,i.jsx)(s.strong,{children:"Inverse Reinforcement Learning (IRL)"})," aim to learn reward functions directly from demonstrations, bypassing some of these manual engineering challenges."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"knowledge-check-",children:"Knowledge Check \u2713"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 1"}),": What problem does reward shaping primarily aim to solve in Reinforcement Learning, especially for complex tasks?\r\na) Overfitting to the training data.\r\nb) The sparse reward problem.\r\nc) Insufficient computational power.\r\nd) The need for large datasets."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 2"}),": Explain the role of the ",(0,i.jsx)(s.code,{children:"sigmoid"})," function in the ",(0,i.jsx)(s.code,{children:"upright_reward"})," component and how it differs from a simple binary threshold reward."]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 3"}),": What are ",(0,i.jsx)(s.code,{children:"UPRIGHT_WEIGHT"}),", ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"}),", and ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," referred to as in the context of reward shaping, and why is their tuning so crucial?"]}),"\n"]}),"\n",(0,i.jsxs)(s.li,{children:["\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Question 4"}),": How does reward shaping, particularly with components like ",(0,i.jsx)(s.code,{children:"control_cost"}),", contribute to designing more efficient and safer robot behaviors in simulation and potentially in the real world?"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"try-this",children:"Try This"}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Experiment with Reward Weights"}),":"]}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Isolate Effects"}),": Set ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"})," and ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," to ",(0,i.jsx)(s.code,{children:"0.0"}),". Observe the robot's behavior with only the ",(0,i.jsx)(s.code,{children:"upright_reward"}),". How long does it stay upright? Does it move?"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Prioritize Speed"}),": Set ",(0,i.jsx)(s.code,{children:"UPRIGHT_WEIGHT"})," and ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," to ",(0,i.jsx)(s.code,{children:"0.0"}),", and increase ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"})," significantly (e.g., ",(0,i.jsx)(s.code,{children:"5.0"}),"). What kind of motion does the robot attempt? Does it become unstable?"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Prioritize Efficiency"}),": Reset weights. Set ",(0,i.jsx)(s.code,{children:"UPRIGHT_WEIGHT"})," to ",(0,i.jsx)(s.code,{children:"1.0"}),", ",(0,i.jsx)(s.code,{children:"VELOCITY_WEIGHT"})," to ",(0,i.jsx)(s.code,{children:"0.5"}),", and make ",(0,i.jsx)(s.code,{children:"CONTROL_COST_WEIGHT"})," a much larger negative value (e.g., ",(0,i.jsx)(s.code,{children:"-0.1"}),"). How does the robot's movement change? Is it slower but smoother?"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Analyze Trade-offs"}),': Discuss how changing these weights creates different "personalities" or behavioral priorities for the robot. What are the trade-offs between speed, stability, and energy consumption?']}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["Sutton, R. S., & Barto, A. A. (2018). ",(0,i.jsx)(s.em,{children:"Reinforcement Learning: An Introduction"})," (2nd ed.). MIT Press. (Chapter 8: Planning and Learning with Tabular Methods, discusses shaping implicitly)."]}),"\n",(0,i.jsxs)(s.li,{children:["Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward transformations: Towards an understanding of reward shaping. ",(0,i.jsx)(s.em,{children:"Proceedings of the Sixteenth International Conference on Machine Learning (ICML)"}),", 278-287. (A seminal paper on potential-based reward shaping)."]}),"\n",(0,i.jsxs)(s.li,{children:["Wiewiora, E. (2003). Principled methods for reward shaping. ",(0,i.jsx)(s.em,{children:"Unpublished Master's Thesis, University of Alberta"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["Andrychowicz, M., et al. (2018). Learning dexterous in-hand manipulation. ",(0,i.jsx)(s.em,{children:"International Conference on Learning Representations (ICLR)"}),". (Showcases complex reward engineering in practice)."]}),"\n"]})]})}function p(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(m,{...e})}):m(e)}}}]);