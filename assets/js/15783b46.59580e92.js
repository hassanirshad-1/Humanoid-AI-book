"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[325],{3379:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"chapter-01-intro/1.4-first-policy","title":"Lesson 1.4: Your First Policy","description":"Congratulations! You\'ve made it to the most exciting lesson in this chapter. You\'re about to write code that makes a robot move. It won\'t be graceful\u2014in fact, it will be chaotic\u2014but it\'s the foundation for everything that follows.","source":"@site/docs/chapter-01-intro/1.4-first-policy.mdx","sourceDirName":"chapter-01-intro","slug":"/chapter-01-intro/1.4-first-policy","permalink":"/Humanoid-AI-book/chapter-01-intro/1.4-first-policy","draft":false,"unlisted":false,"editUrl":"https://github.com/hassanirshad-1/Humanoid-AI-book/tree/main/docs/chapter-01-intro/1.4-first-policy.mdx","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1.3: Entering \\"The Matrix\\"","permalink":"/Humanoid-AI-book/chapter-01-intro/1.3-simulation-setup"},"next":{"title":"2.1 What is a Reward","permalink":"/Humanoid-AI-book/chapter-02-reward-signal/2.1-what-is-a-reward"}}');var i=t(4848),o=t(8453);const s={},l="Lesson 1.4: Your First Policy",a={},c=[{value:"The Concrete Hook: Making It Move",id:"the-concrete-hook-making-it-move",level:2},{value:"The Intuition: The Action Loop",id:"the-intuition-the-action-loop",level:2},{value:"Step 1: SENSE",id:"step-1-sense",level:3},{value:"Step 2: PLAN",id:"step-2-plan",level:3},{value:"Step 3: ACT",id:"step-3-act",level:3},{value:"Step 4: REPEAT",id:"step-4-repeat",level:3},{value:"The Analogy: Reflexes and Habits",id:"the-analogy-reflexes-and-habits",level:2},{value:"The Code: Random Policy in Action",id:"the-code-random-policy-in-action",level:2},{value:"The Complete Script",id:"the-complete-script",level:3},{value:"Running the Script",id:"running-the-script",level:3},{value:"Understanding Why Random Fails",id:"understanding-why-random-fails",level:2},{value:"The Probability Problem",id:"the-probability-problem",level:3},{value:"The Temporal Problem",id:"the-temporal-problem",level:3},{value:"The Solution: Learning",id:"the-solution-learning",level:3},{value:"Extending the Random Policy",id:"extending-the-random-policy",level:2},{value:"Experiment 1: Zero Policy",id:"experiment-1-zero-policy",level:3},{value:"Experiment 2: Constant Policy",id:"experiment-2-constant-policy",level:3},{value:"Experiment 3: Sine Wave Policy",id:"experiment-3-sine-wave-policy",level:3},{value:"The Bigger Picture",id:"the-bigger-picture",level:2},{value:"Knowledge Check \u2713",id:"knowledge-check-",level:2},{value:"Try This: Challenges",id:"try-this-challenges",level:2},{value:"Summary",id:"summary",level:2},{value:"What&#39;s Next?",id:"whats-next",level:2},{value:"Chapter 1 Exercises (Optional)",id:"chapter-1-exercises-optional",level:2}];function h(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"lesson-14-your-first-policy",children:"Lesson 1.4: Your First Policy"})}),"\n",(0,i.jsx)(n.p,{children:"Congratulations! You've made it to the most exciting lesson in this chapter. You're about to write code that makes a robot move. It won't be graceful\u2014in fact, it will be chaotic\u2014but it's the foundation for everything that follows."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"the-concrete-hook-making-it-move",children:"The Concrete Hook: Making It Move"}),"\n",(0,i.jsx)(n.p,{children:"You have a robot loaded in simulation. It's standing there, motionless, probably about to fall over due to gravity. Now it's time to give it a brain."}),"\n",(0,i.jsx)(n.p,{children:"In this lesson, you will:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'Understand the "action loop" that powers all robot control'}),"\n",(0,i.jsx)(n.li,{children:"Write a simple policy that sends random commands"}),"\n",(0,i.jsx)(n.li,{children:"Watch the robot flail and fall (on purpose!)"}),"\n",(0,i.jsx)(n.li,{children:"Understand why random policies fail and learning is necessary"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"By the end, you'll have implemented the core control loop used in every robotics system, from toy robots to industrial manipulators to humanoid androids."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"the-intuition-the-action-loop",children:"The Intuition: The Action Loop"}),"\n",(0,i.jsxs)(n.p,{children:["Recall from Lesson 1.1 that a policy ",(0,i.jsx)(n.code,{children:"\u03c0(a|s)"})," is a function that takes state and outputs action. But how do we actually use this in a running robot?"]}),"\n",(0,i.jsxs)(n.p,{children:["The answer is the ",(0,i.jsx)(n.strong,{children:"action loop"}),", also called the ",(0,i.jsx)(n.strong,{children:"sense-plan-act cycle"}),". It's the heartbeat of every robot:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\r\n\u2502                                 \u2502\r\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502  \u2502  SENSE  \u2502 \u2500\u2500\u2192  \u2502  PLAN   \u2502  \u2502\r\n\u2502  \u2502(sensors)\u2502      \u2502(policy) \u2502  \u2502\r\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2502       \u2191                \u2502       \u2502\r\n\u2502       \u2502                \u2193       \u2502\r\n\u2502       \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\r\n\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502   ACT   \u2502  \u2502\r\n\u2502                  \u2502(motors) \u2502  \u2502\r\n\u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\r\n\u2502                                 \u2502\r\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-1-sense",children:"Step 1: SENSE"}),"\n",(0,i.jsx)(n.p,{children:"Read the current state from the robot's sensors:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Joint angles and velocities from encoders"}),"\n",(0,i.jsx)(n.li,{children:"Orientation and acceleration from the IMU"}),"\n",(0,i.jsx)(n.li,{children:"Contact forces from foot sensors"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-2-plan",children:"Step 2: PLAN"}),"\n",(0,i.jsx)(n.p,{children:"Feed this state into the policy and compute an action:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Run the neural network (or any control algorithm)"}),"\n",(0,i.jsx)(n.li,{children:"Output the desired motor commands"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-3-act",children:"Step 3: ACT"}),"\n",(0,i.jsx)(n.p,{children:"Send the action to the motors:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The physics engine applies the commanded torques"}),"\n",(0,i.jsx)(n.li,{children:"The robot moves"}),"\n",(0,i.jsx)(n.li,{children:"The world changes"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-4-repeat",children:"Step 4: REPEAT"}),"\n",(0,i.jsx)(n.p,{children:"Go back to step 1. This happens 100-1000 times per second, continuously, for as long as the robot is running."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"the-analogy-reflexes-and-habits",children:"The Analogy: Reflexes and Habits"}),"\n",(0,i.jsx)(n.p,{children:"Consider a simple human reflex: touching a hot stove."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sense"}),": Your skin's heat receptors detect dangerously high temperature"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan"}),": Your spinal cord (not even your brain!) computes a response"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Act"}),": Your arm muscles contract to pull your hand away"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This entire loop happens in about 50 milliseconds\u2014faster than conscious thought."}),"\n",(0,i.jsx)(n.p,{children:"Now think about a more complex skill like riding a bicycle:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sense"}),": Your inner ear detects tilting; your eyes see the road"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Plan"}),": Your cerebellum computes tiny corrections (learned over years)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Act"}),": Your muscles adjust the handlebar angle and your body weight"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'These are learned policies running in your nervous system. The "training data" was all those times you fell off the bike as a child. Each fall provided feedback that updated your internal policy.'}),"\n",(0,i.jsx)(n.p,{children:"For our robot, the action loop is identical. We're building an artificial sense-plan-act cycle, and in later chapters, we'll train the \"plan\" step using machine learning."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"the-code-random-policy-in-action",children:"The Code: Random Policy in Action"}),"\n",(0,i.jsx)(n.p,{children:"Let's implement this! We'll start with the simplest possible policy: one that ignores the state entirely and outputs random actions."}),"\n",(0,i.jsx)(n.h3,{id:"the-complete-script",children:"The Complete Script"}),"\n",(0,i.jsxs)(n.p,{children:["Save this as ",(0,i.jsx)(n.code,{children:"action_loop.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'"""\r\nLesson 1.4: Your First Policy\r\nA random policy that makes the robot flail around.\r\nThis demonstrates the action loop structure we\'ll use throughout the book.\r\n"""\r\n\r\nimport mujoco\r\nimport mujoco.viewer\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\n\r\ndef random_policy(model, data):\r\n    """\r\n    The simplest possible policy: random actions.\r\n    \r\n    Args:\r\n        model: MuJoCo model (contains robot structure)\r\n        data: MuJoCo data (contains current state)\r\n    \r\n    Returns:\r\n        action: numpy array of shape (model.nu,) with values in [-1, 1]\r\n    """\r\n    # model.nu = number of actuators (motors)\r\n    action = np.random.uniform(low=-1.0, high=1.0, size=model.nu)\r\n    return action\r\n\r\n\r\ndef get_state(model, data):\r\n    """\r\n    Extract the state vector from MuJoCo data.\r\n    \r\n    Returns a dictionary with all the information a policy might need.\r\n    """\r\n    return {\r\n        \'joint_positions\': data.qpos.copy(),      # All joint angles\r\n        \'joint_velocities\': data.qvel.copy(),     # All joint velocities\r\n        \'time\': data.time,                         # Simulation time\r\n        \'center_of_mass\': data.subtree_com[0].copy(),  # CoM position\r\n    }\r\n\r\n\r\ndef main():\r\n    # --- Setup ---\r\n    script_dir = os.path.dirname(os.path.abspath(__file__))\r\n    model_path = os.path.join(script_dir, "unitree_g1.xml")\r\n    \r\n    print("="*50)\r\n    print("Lesson 1.4: Your First Policy")\r\n    print("="*50)\r\n    \r\n    # Load model\r\n    try:\r\n        model = mujoco.MjModel.from_xml_path(model_path)\r\n        data = mujoco.MjData(model)\r\n        print(f"\u2713 Model loaded: {model.nu} actuators, {model.nq} DOF")\r\n    except Exception as e:\r\n        print(f"\u2717 Error: {e}")\r\n        return\r\n    \r\n    # --- Statistics tracking ---\r\n    step_count = 0\r\n    total_reward = 0.0\r\n    episode_start_time = time.time()\r\n    \r\n    # --- Main loop ---\r\n    with mujoco.viewer.launch_passive(model, data) as viewer:\r\n        viewer.cam.azimuth = 90\r\n        viewer.cam.elevation = -15\r\n        viewer.cam.distance = 3.0\r\n        viewer.cam.lookat[:] = [0.0, 0.0, 0.75]\r\n        \r\n        print("\\nSimulation running with RANDOM POLICY")\r\n        print("The robot will flail and fall - this is expected!")\r\n        print("Press Ctrl+C or close window to stop.\\n")\r\n        \r\n        try:\r\n            while viewer.is_running():\r\n                step_start = time.time()\r\n                \r\n                # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\r\n                # THE ACTION LOOP - This is the core of all robotics!\r\n                # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\r\n                \r\n                # 1. SENSE: Get current state\r\n                state = get_state(model, data)\r\n                \r\n                # 2. PLAN: Compute action from policy\r\n                action = random_policy(model, data)\r\n                \r\n                # 3. ACT: Apply action to motors\r\n                data.ctrl[:] = action\r\n                \r\n                # 4. STEP: Advance physics simulation\r\n                mujoco.mj_step(model, data)\r\n                \r\n                # \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\r\n                \r\n                # --- Compute reward (for monitoring) ---\r\n                height = state[\'center_of_mass\'][2]\r\n                reward = height  # Simple reward: higher is better\r\n                total_reward += reward\r\n                step_count += 1\r\n                \r\n                # Print stats every 500 steps (~1 second at 500Hz)\r\n                if step_count % 500 == 0:\r\n                    avg_reward = total_reward / step_count\r\n                    elapsed = time.time() - episode_start_time\r\n                    print(f"Step {step_count:5d} | "\r\n                          f"Height: {height:.3f}m | "\r\n                          f"Avg Reward: {avg_reward:.3f} | "\r\n                          f"Time: {elapsed:.1f}s")\r\n                \r\n                # Update viewer\r\n                viewer.sync()\r\n                \r\n                # Sleep to maintain real-time\r\n                time_to_sleep = model.opt.timestep - (time.time() - step_start)\r\n                if time_to_sleep > 0:\r\n                    time.sleep(time_to_sleep)\r\n                    \r\n        except KeyboardInterrupt:\r\n            print("\\n\\nSimulation stopped by user.")\r\n        \r\n        print(f"\\nFinal Statistics:")\r\n        print(f"  Total steps: {step_count}")\r\n        print(f"  Total reward: {total_reward:.2f}")\r\n        print(f"  Average reward: {total_reward/max(step_count, 1):.4f}")\r\n\r\n\r\nif __name__ == "__main__":\r\n    main()\n'})}),"\n",(0,i.jsx)(n.h3,{id:"running-the-script",children:"Running the Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"python action_loop.py\n"})}),"\n",(0,i.jsx)(n.p,{children:"Watch the robot! It will:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Jerk around randomly"}),"\n",(0,i.jsx)(n.li,{children:"Probably fall over almost immediately"}),"\n",(0,i.jsx)(n.li,{children:"Maybe do some accidental flips"}),"\n",(0,i.jsx)(n.li,{children:"Definitely not walk"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"This is expected and correct!"})," A random policy knows nothing about physics, balance, or locomotion. It's just sending noise to the motors."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"understanding-why-random-fails",children:"Understanding Why Random Fails"}),"\n",(0,i.jsx)(n.p,{children:"Let's think about why random actions don't work:"}),"\n",(0,i.jsx)(n.h3,{id:"the-probability-problem",children:"The Probability Problem"}),"\n",(0,i.jsxs)(n.p,{children:["For a 4-DOF robot with continuous actions, the action space is 4-dimensional. To stand upright, the robot needs to output a very specific combination of torques\u2014maybe something like ",(0,i.jsx)(n.code,{children:"[0.1, -0.2, 0.15, -0.18]"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"What's the probability that random sampling produces exactly that? Essentially zero."}),"\n",(0,i.jsx)(n.p,{children:"For a 23-DOF robot, it's even worse. Randomly sampling the correct action is like trying to guess a 23-digit password."}),"\n",(0,i.jsx)(n.h3,{id:"the-temporal-problem",children:"The Temporal Problem"}),"\n",(0,i.jsxs)(n.p,{children:["Even if you accidentally hit the right action for one timestep, you need to hit the right action for the ",(0,i.jsx)(n.em,{children:"next"})," timestep too. And the next. And the next."]}),"\n",(0,i.jsx)(n.p,{children:"Walking requires coordinated sequences of hundreds of actions. The probability of randomly generating a valid walking sequence is astronomically small."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Rough calculation\r\naction_space_volume = 2 ** 23  # Each dimension from -1 to 1\r\nsteps_to_walk_one_step = 100\r\nprobability_of_random_walk = (1 / action_space_volume) ** steps_to_walk_one_step\r\n# = basically 0\n"})}),"\n",(0,i.jsx)(n.h3,{id:"the-solution-learning",children:"The Solution: Learning"}),"\n",(0,i.jsxs)(n.p,{children:["This is why we need ",(0,i.jsx)(n.strong,{children:"reinforcement learning"}),". Instead of random sampling, we:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Try random actions initially"}),"\n",(0,i.jsx)(n.li,{children:"See which ones lead to reward (staying upright, moving forward)"}),"\n",(0,i.jsx)(n.li,{children:"Gradually adjust the policy to output more rewarding actions"}),"\n",(0,i.jsx)(n.li,{children:"After millions of trials, the policy learns what works"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"We'll implement this learning process starting in Chapter 3."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"extending-the-random-policy",children:"Extending the Random Policy"}),"\n",(0,i.jsx)(n.p,{children:"Before we move on, let's try a few modifications to develop intuition:"}),"\n",(0,i.jsx)(n.h3,{id:"experiment-1-zero-policy",children:"Experiment 1: Zero Policy"}),"\n",(0,i.jsx)(n.p,{children:"What happens if we send zero torque to all motors?"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def zero_policy(model, data):\r\n    """Do nothing - let gravity take over."""\r\n    return np.zeros(model.nu)\n'})}),"\n",(0,i.jsx)(n.p,{children:"Try it! The robot will collapse immediately because there's no force countering gravity."}),"\n",(0,i.jsx)(n.h3,{id:"experiment-2-constant-policy",children:"Experiment 2: Constant Policy"}),"\n",(0,i.jsx)(n.p,{children:"What about fixed, non-zero actions?"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def constant_policy(model, data):\r\n    """Send the same action every time."""\r\n    return np.array([0.5, -0.5, 0.5, -0.5])  # Arbitrary constant\n'})}),"\n",(0,i.jsx)(n.p,{children:"The robot will move in a consistent (but probably wrong) way. It might tip over slowly instead of instantly."}),"\n",(0,i.jsx)(n.h3,{id:"experiment-3-sine-wave-policy",children:"Experiment 3: Sine Wave Policy"}),"\n",(0,i.jsx)(n.p,{children:"What about smooth, cyclic actions?"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def sine_policy(model, data):\r\n    """Actions vary smoothly over time."""\r\n    t = data.time\r\n    return np.array([\r\n        np.sin(2 * t),\r\n        np.sin(2 * t + np.pi/2),\r\n        np.sin(2 * t + np.pi),\r\n        np.sin(2 * t + 3*np.pi/2),\r\n    ])\n'})}),"\n",(0,i.jsx)(n.p,{children:"This produces rhythmic, alternating movements. It's still not walking, but it's more structured than random noise."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"the-bigger-picture",children:"The Bigger Picture"}),"\n",(0,i.jsx)(n.p,{children:"What you've built today is more significant than it might seem. This action loop\u2014sense, plan, act, repeat\u2014is the foundation of:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Industrial robots"})," in factories"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Self-driving cars"})," on the road"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Drones"})," in the sky"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Surgical robots"})," in operating rooms"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Humanoid robots"})," in research labs"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:'The only difference is what goes in the "plan" step. In this lesson, it was random noise. In real systems, it\'s sophisticated algorithms trained on millions of examples.'}),"\n",(0,i.jsxs)(n.p,{children:["Over the next chapters, we'll replace ",(0,i.jsx)(n.code,{children:"random_policy()"})," with increasingly intelligent alternatives until we have a robot that can actually walk."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"knowledge-check-",children:"Knowledge Check \u2713"}),"\n",(0,i.jsx)(n.p,{children:"Before moving to Chapter 2, make sure you can answer:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"What are the three steps"})," of the action loop?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Why can't random policies"})," learn to walk?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["What is ",(0,i.jsx)(n.code,{children:"model.nu"})]})," in MuJoCo?"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"What would happen"})," if we removed the ",(0,i.jsx)(n.code,{children:"time.sleep()"})," call?"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"try-this-challenges",children:"Try This: Challenges"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Modify the reward function"}),': Change it from height-based to velocity-based. How does this affect what the robot "tries" to do?']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add action smoothing"}),": Currently, actions change completely every timestep. Try averaging with the previous action:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"action = 0.9 * previous_action + 0.1 * random_policy(model, data)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Does the robot move more smoothly?"}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Track falling"}),": Add code to detect when the robot has fallen (height < 0.3m). Count how many steps it takes to fall with different policies."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, you learned:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Lesson"}),(0,i.jsx)(n.th,{children:"Key Concept"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.1"}),(0,i.jsxs)(n.td,{children:["Policy ",(0,i.jsx)(n.code,{children:"\u03c0(a|s)"})," maps state to action; MDP formalism"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.2"}),(0,i.jsx)(n.td,{children:"Robots have actuators, sensors, and compute; DOF and state vectors"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.3"}),(0,i.jsx)(n.td,{children:"MuJoCo setup; loading models and running simulations"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"1.4"}),(0,i.jsx)(n.td,{children:"The action loop (sense-plan-act); why learning is needed"})]})]})]}),"\n",(0,i.jsx)(n.p,{children:"You now have a working simulation environment and understand the fundamental concepts of robot control. Most importantly, you've seen firsthand why we need machine learning\u2014random policies simply cannot produce intelligent behavior."}),"\n",(0,i.jsxs)(n.p,{children:["In the next chapter, we'll introduce ",(0,i.jsx)(n.strong,{children:"reward shaping"})," and ",(0,i.jsx)(n.strong,{children:"policy gradients"}),"\u2014the tools that let robots learn from experience."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Chapter 2: Reward Design"}),' will teach you how to define what "good behavior" means mathematically. You\'ll learn that the reward function is just as important as the learning algorithm\u2014maybe more so.']}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["\u2192 Continue to ",(0,i.jsx)(n.a,{href:"/Humanoid-AI-book/chapter-02-reward-signal/2.1-what-is-a-reward",children:"Chapter 2: The Reward Signal"})]})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"chapter-1-exercises-optional",children:"Chapter 1 Exercises (Optional)"}),"\n",(0,i.jsx)(n.p,{children:"If you want extra practice before moving on:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Full State Logger"}),": Modify ",(0,i.jsx)(n.code,{children:"action_loop.py"})," to save the complete state trajectory to a CSV file. Plot joint angles over time using matplotlib."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Policy Zoo"}),": Implement 5 different deterministic policies (zero, constant, sine, sawtooth, etc.) and compare how long each takes to fall."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Model Surgery"}),": Modify the MJCF file to add an arm to the robot. How does this change ",(0,i.jsx)(n.code,{children:"model.nu"})," and ",(0,i.jsx)(n.code,{children:"model.nq"}),"?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-Time Visualization"}),": Add a matplotlib plot that updates in real-time, showing the robot's height over the last 100 timesteps."]}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>l});var r=t(6540);const i={},o=r.createContext(i);function s(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);